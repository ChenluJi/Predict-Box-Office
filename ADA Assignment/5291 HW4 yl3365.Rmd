---
title: ""
author: ""
date: ""
output:
  pdf_document:
    fig_width: 8
    fig_height: 6
    fig_caption: true
fontsize: 10pt
---

#GR5291&nbsp;&nbsp;&nbsp;&nbsp;Advanced Data Analysis&nbsp;&nbsp;&nbsp;&nbsp;Homework 4

#*YIFEI LANG&nbsp;&nbsp;&nbsp;&nbsp;UNI:yl3365*

##
##Question 1

###i)

We can load the data and do some exploratory data analysis first to get some basic information about the data. 

```{r}
library(MASS)
data("birthwt")
head(birthwt, obs = 5)
dim(birthwt)
summary(birthwt)
```

Then we can fit a multiple linear regression model by using the following code:

```{r}
reg1 = lm(bwt~age+lwt+race+smoke+ptl+ht+ui+ftv,data=birthwt)
summary(reg1)
confint(reg1, level=0.95) # CIs for model parameters 
```

From the results above, the model is $bwt = 3129.46-0.27*age+3.44*lwt-188.49*race-358.46*smoke-51.15*ptl-600.65*ht-511.25*ui-15.54*ftv$. However, we can find that the *'age'*, *'ptl'* and *'ftv'* are not significant in the model. To test whether there is **Multicollinearity** within the predictors. There are many methods:

```{r warning=FALSE}
library(car)
birth = cbind(birthwt$bwt, birthwt[,c(-1,-10)])
cor(birth)
scatterplotMatrix(birth,var.labels = colnames(birth),diagonal = "boxplot")
sqrt(vif(reg1))
eigen(cor(birth))$value
```

From the correlation matrix, we can find most of the correlations between the predictors are low. There is no correlation larger than 0.5, so that it is hard to say that multicollinearity exists. Then when we take a look at the scatterplot matrix, we can find that most of the predictors are categotical variables. So it is reasonable to find no multicollinearity. Finally, to have a numerical test about the collineartiy, we can check the sqare root of VIF, which also prove that there is no multicollinearity between predictors because all of the sqare root of VIF are less than 2. The eigenvalue of the correlaiton matrix also prove our conclusion since there is no eigenvalue near 0.

###ii)

To conduce ridge regression, we can use the `fit.ridge` and `cv.ridge` to select the best shrinkage parameter and the `lm.ridge` in the library `MASS` to find the estimates of parameters.

```{r include=FALSE}
library(glmnet)
```


```{r warning=FALSE}
attach(birthwt)
fit.ridge = glmnet(cbind(age,lwt,race,smoke,ptl,ht,ui,ftv),bwt,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
```

From the fist plot of coefficients against Log Lambda, we can find that when the shrinkage parameter is small, which means the model is close to OLS model, some of the coefficients are pretty large. And as the lambda increasing, the coefficients are concentrated. To find out when can we get the best model, we can use `cv.ridge` to conduct cross-validation to the data.

```{r warning=FALSE}
cv.ridge = cv.glmnet(cbind(age,lwt,race,smoke,ptl,ht,ui,ftv),bwt,alpha=0)
plot(cv.ridge)
```

From the results above, we can find that in the beginning, the mean squared error is very high, and the coefficients are restricted to be too small, and then at some point, it kind of levels off. This seems to indicate that the full model is doing a good job.

There's two vertical lines.

- The left one is at the minimum Mean-Squared Error

- The other vertical line is within one standard error of the minimum. The second line is a slightly more restricted model that does almost as well as the minimum.

At the top of the plot, we actually see how many non-zero variables coefficients are in the model. There's all 8 variables in the model and no coefficient is zero. There's a coefficient function extractor that works on a cross validation object and pick the coefficient vector corresponding to the best model:

```{r}
coef(cv.ridge)
```

From the results above, the model is $bwt = 2914.77+1.18*age+0.45*lwt-16.65*race-30.72*smoke-22.60*ptl-49.11*ht-60.90*ui+3.53*ftv$. Compare this model to the linear regression results, we can find that all the coefficient were shrunk, and all 8 predictors are included which is not same as the result of linear regression. The sign of the coeffecients of *'age'* and *'ftv'* are changed.

##
##Question 2

###Lasso

We can do lasso regression by using the similar codes as ridge regression:

```{r}
fit.lasso = glmnet(cbind(age,lwt,race,smoke,ptl,ht,ui,ftv),bwt,alpha=1)
plot(fit.lasso,xvar="lambda",label=TRUE)
```

From the plot of coefficients against Log Lambda, we can find that when the shrinkage parameter is small, which means the model is the same as OLS model, some of the coefficients are pretty large. And as the lambda increasing, the coefficients are concentrated. The plot has various choices. The deviance shows the percentage of deviance explained, (equivalent to r squared in case of regression)

```{r}
plot(fit.lasso,xvar="dev",label=TRUE)
```

A lot of the r squared was explained for quite heavily shrunk coefficients. And towards the end, with a relatively small increase in r squared from between 0.1 and 0.2, coefficients grow very large. This may be an indication that the end of the path is overfitting

To find out when can we get the best model, we can use `cv.lasso` to conduct cross-validation to the data.

```{r warning=FALSE}
cv.lasso = cv.glmnet(cbind(age,lwt,race,smoke,ptl,ht,ui,ftv),bwt,alpha=1)
plot(cv.lasso)
```

The fit.lasso will have the whole path of coefficients, which is 100 coefficient vectors dependent on each value, indexed by different values of lambda. We can find when we do lasso regression, it shows that two optimal selection of the predictors are the model with 6 predictors and 5 predictors.

To find out which is the best model, there's a coefficient function extractor that works on a cross validation object and pick the coefficient vector corresponding to the best model:

```{r}
coef(cv.lasso)
```

The output above has 6 non-zero coefficients which shows that the function has chosen the first vertical line on the cross-validation plot. Which is a similar result as the OLS linear regression. The model is $bwt = 3070.09+0.22*lwt-44.33*race-78.54*smoke-36.93*ht-265.21*ui$.

###Stepwise

We can do stepwise selection towards the model by `stepAIC` in the library `MASS`. 

```{r}
fit.step = stepAIC(reg1, direction="both")
fit.step$anova 
fit.step$coefficients
```

We can find the results gave a similar model as we obtained from the lasso regression, but with larger coefficients. This phenomenon proved the shrinkage property of the lasso method. The model is $bwt = 3104.44+3.43*lwt-187.85*race-366.13*smoke-595.82*ht-523.42*ui$.

##
##Question 3

**Rating of performance of procedures below:**

*1 = Good, 2 = Fair, 3= Poor*

```{r echo=FALSE, results='asis'}
library(knitr)
a = matrix(c(3,1,1,1,
             3,1,3,1,
             1,3,3,3,
             1,3,1,1,
              NA, NA, NA, NA,
             1,2,2,2,
             1,3,2,2,
             2,3,1,1), nrow = 8, ncol = 4, byrow = T)
rownames(a) = c("1. Performance when p >> n",
                "2. Performance under multicollinearity",
                "3. Unbiased estimators",
                "4. Model selection capability",
                "5. Simplicity: ",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    Computation",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    Inference",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    Interpretation")
colnames(a) = c("OLS","Ridge","LASSO","Elastic Net")
kable(a)

```

The properties for different methods:

###OLS (Ordinary Least Square)

**PROS**

- OLS has the minimum MSE among unbiased linear estimator
- Explicit form
- Computation $O(np^2)$
- Confidence Interval, Significance of Coefficient can be calculated

**CONS**

- Multicollinearity leads to high variance of estimators
- Requires $n>p$
- Prediction error increases linearly as a function of p
- Hard to interpret when the number of predictors is large

###Ridge Regression

**PROS**

- $p>>n$
- Explicit solution
- Multicollinearity
- Biased but smaller variance and smaller MSE

**CONS**

- Shrink coefficients to zero but can not produce a parsimonious model
- Not good for variable selection (model selection)
- A ridge solution can be hard to interpret because it is not sparse

###Lasso Regression

**PROS**

- Allow $p>>n$
- Enforce sparcity in parameters
- Quadratic programming problem 
- When $/lambda$ goes to 0, OLS solution
- Good for variable selection

**CONS**

- If a group of predictors are highly correlated among themselves, LASSO tends to pick only one of them and shrink others to zero

- Can not do grouped selection

###Elastic Net

**PROS**

- It shares all of the advantages of Ridge and LASSO regression
- No limitation on the number of selected variable
- Enforce sparcity in parameters
- Encourge grouping effect in the presence of highly correlated predictors

**CONS**

- Naive elastic net suffers from double shrinkage



